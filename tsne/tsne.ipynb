{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b709aa-5ee1-48a6-84ff-5dfbec507227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, math, collections, typing, warnings\n",
    "import imblearn, matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import decomposition, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62730b3a-1966-4991-8ddd-68cb40d70c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54dcbb-e84d-4b92-ae82-6937373ade19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Global variables\n",
    "methods = {'SMOTETomek': imblearn.combine.SMOTETomek, \n",
    "           'RandomOver': imblearn.over_sampling.RandomOverSampler,\n",
    "           'ADASYN': imblearn.over_sampling.ADASYN,\n",
    "           'RandomUnder': imblearn.under_sampling.RandomUnderSampler,\n",
    "           'none': None,}\n",
    "INPUT_NAME = 'text'\n",
    "OUTPUT_NAMES = ['Binary', '4-type', '5-type']\n",
    "THRESHOLD_PCT = 0.5 #minimum percent of text that must be taken up by a particular word\n",
    "PCA_DIMS = 50 #dimensions to reduce to with PCA before applying t-SNE\n",
    "TSNE_DIMS = 2\n",
    "TSNE_PERPLEXITY = 40\n",
    "TSNE_ITER = 300 #number of iterations for t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5465652-8c5e-4f51-99d6-0dd89041e768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Word representations\n",
    "def bow(text: str, wordlist: list[str]) -> list[int]:\n",
    "    '''Represent text with bag of words.'''\n",
    "    return [int(word in text) for word in wordlist]\n",
    "\n",
    "def freq(text: str, wordlist: list[str]) -> list[float]:\n",
    "    '''Represent text with frequencies.'''\n",
    "    text = text.split()\n",
    "    counter = collections.Counter(text)\n",
    "    length = len(text)\n",
    "    return [value_or_zero(word, counter)/length for word in wordlist]\n",
    "\n",
    "def tfidf(text: str, word_dict: dict[str, float]) -> list[float]:\n",
    "    '''Represent text with TF-IDF'''\n",
    "    text = text.split()\n",
    "    counter = collections.Counter(text)\n",
    "    length = len(text)\n",
    "    frequencies = [value_or_zero(word, counter)/length for word in word_dict]\n",
    "    return [word_dict[word] * frequencies[i] for (i, word) in enumerate(word_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef07d5a-bc41-4b5f-81b7-28bc9a7abbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data formatting and wrangling functions\n",
    "def get_all_text(df: pd.DataFrame, output_index: int) -> str:\n",
    "    '''Get a string with all text from a pandas DataFrame.'''\n",
    "    return ' '.join(list(df[INPUT_NAME]))\n",
    "\n",
    "def combine_answers(df: pd.DataFrame, names: list[str]) -> pd.DataFrame:\n",
    "    '''Combine the answers to get the 5-type class'''\n",
    "    columns = list(df.columns.values)\n",
    "    data_type, classification = columns.index(names[0]), columns.index(names[1])\n",
    "    df[OUTPUT_NAMES[2]] = df.apply(lambda x: (x[data_type] if x[data_type] is int else 0) + x[classification], axis=1)\n",
    "    return df\n",
    "\n",
    "def remove_uncommon_words(wordlist: list[str], text_counter: dict[str, int]) -> list[str]:\n",
    "    '''Remove words that are uncommon, below a certain threshold.'''\n",
    "    return [word for word in wordlist if text_counter[word]*100/len(wordlist) >= THRESHOLD_PCT]\n",
    "\n",
    "def get_wordlist(df: pd.DataFrame, output_index: str) -> tuple[list[str], dict[str, int]]:\n",
    "    '''Get the wordlist for a dataframe.'''\n",
    "    all_text_split = get_all_text(df, output_index).split()\n",
    "    all_text_counter = collections.Counter(all_text_split)\n",
    "    return remove_uncommon_words(list(set(all_text_split)), all_text_counter), all_text_counter\n",
    "\n",
    "def get_worddict(df: pd.DataFrame, output_index: str, frequency_counter: dict[str, int], \n",
    "                 wordlist: list[str]) -> dict[str, float]:\n",
    "    '''Get the word dict for a dataframe, for the TF-IDF encoding.'''\n",
    "    all_text_split = get_all_text(df, output_index).split()\n",
    "    all_text_counter = collections.Counter(all_text_split)\n",
    "    word_dict = dict()\n",
    "    for word in wordlist:\n",
    "        value = math.log(len(wordlist)/all_text_counter[word])\n",
    "        word_dict[word] = value\n",
    "    return word_dict\n",
    "\n",
    "def resample(inputs: np.ndarray, outputs: np.ndarray, resampling_method: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    '''Resample a dataset with a resampling method.'''\n",
    "    try:\n",
    "        if resampling_method == 'aug':\n",
    "            pass\n",
    "        elif resampling_method == 'ADASYN':\n",
    "            inputs, outputs = methods[resampling_method](sampling_strategy='minority').fit_resample(inputs, outputs)\n",
    "        elif resampling_method != 'none':\n",
    "            inputs, outputs = methods[resampling_method]().fit_resample(inputs, outputs)\n",
    "    except ValueError: #sometimes it doesn't generate any new samples, which gives value error. In that case, return original.\n",
    "        pass\n",
    "    return inputs, outputs\n",
    "\n",
    "def pca_reduce(array: np.ndarray) -> np.ndarray:\n",
    "    '''Reduce the dimensions to a smaller number using PCA.'''\n",
    "    pca = decomposition.PCA(n_components=PCA_DIMS)\n",
    "    return np.array(pca.fit_transform(array))\n",
    "\n",
    "def tsne_reduce(array: np.ndarray) -> np.ndarray:\n",
    "    '''Reduce to two-dimensional data using PCA and t-SNE.'''\n",
    "    array = pca_reduce(array) #get reasonable number of dimensions to then use t-SNE on.\n",
    "    tsne = manifold.TSNE(n_components=TSNE_DIMS, perplexity=TSNE_PERPLEXITY, n_iter=TSNE_ITER)\n",
    "    return np.array(tsne.fit_transform(array))\n",
    "\n",
    "def value_or_zero(key: str, temp_dict: dict[str, int]) -> int:\n",
    "    '''Return value in a dictionary or zero if key not in dictionary.'''\n",
    "    return 0 if key not in temp_dict else temp_dict[key]\n",
    "\n",
    "def delete_punctuation(input_string: str) -> str:\n",
    "    '''Delete the punctuation from a string.'''\n",
    "    return ''.join(char for char in input_string if char not in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa05b6-49ae-4aa7-8c33-e3de936d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary visualization functions\n",
    "def visualize_rep_method(inputs: np.ndarray, outputs: np.ndarray, name: str, ax: matplotlib.axes.Axes) -> None:\n",
    "    '''Visualize a particular representation and resampling method.'''\n",
    "    inputs = tsne_reduce(inputs)\n",
    "    range_ten = range(10)\n",
    "    unique_items = [unique for unique in list(set(outputs)) if unique in range_ten] #get rid of nans\n",
    "    for unique_output in unique_items:\n",
    "        temp_inputs = np.array([item for (i, item) in enumerate(inputs) if outputs[i] == unique_output])\n",
    "        ax.scatter(temp_inputs[:, 0], temp_inputs[:, 1], label=unique_output, s=0.75)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "\n",
    "def visualize_method(df: pd.DataFrame, method: str, task: str, task_index: int) -> None:\n",
    "    '''Visualize each representation using a resampling method.'''\n",
    "    wordlist, frequency_counter = get_wordlist(df, task_index)\n",
    "    word_dict = get_worddict(df, task_index, frequency_counter, wordlist)\n",
    "    rep_methods = [{'name': 'bow', 'function': bow, 'wordlist': wordlist}, #methods for representing text\n",
    "                   {'name': 'freq', 'function': freq, 'wordlist': wordlist},\n",
    "                   {'name': 'tfidf', 'function': tfidf, 'wordlist': word_dict},]\n",
    "    \n",
    "    fig, axs = plt.subplots(len(rep_methods))\n",
    "    for (i, rep_method) in enumerate(rep_methods):\n",
    "        rep_name, rep_func, rep_list = rep_method['name'], rep_method['function'], rep_method['wordlist']\n",
    "        inputs = df[INPUT_NAME].apply(lambda x: rep_func(x, rep_list)).to_numpy()\n",
    "        inputs = np.array([np.array(x) for x in inputs]) #it doesn't do this automatically for some reason\n",
    "        outputs = df[task].to_numpy()\n",
    "        outputs = np.array([np.array(x) for x in outputs])\n",
    "        inputs, outputs = resample(inputs, outputs, method)\n",
    "        visualize_rep_method(inputs, outputs, rep_name, axs[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc445dd4-773a-4da9-808e-b73c90866e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(filename: str, input_name: str, output_names: list[str], task: str, task_index: int, augmented: bool = False) -> None:\n",
    "    '''Generate all visualizations for a datafile and task given their names.'''\n",
    "    df = pd.read_csv(filename)\n",
    "    #ensure i/o columns all have the same names\n",
    "    df[INPUT_NAME] = df[input_name].astype(str)\n",
    "    df = combine_answers(df, output_names)\n",
    "    for (i, incorrect_output_name) in enumerate(output_names):\n",
    "        df[OUTPUT_NAMES[i]] = df[incorrect_output_name]\n",
    "        df[OUTPUT_NAMES[i]]\n",
    "    \n",
    "    if augmented:\n",
    "        visualize_method(df, 'aug', task, i)\n",
    "    else:\n",
    "        for method in methods:\n",
    "            print(f'RESAMPLING METHOD: {method}')\n",
    "            visualize_method(df, method, task, i)\n",
    "\n",
    "def file_generate(filename: str, names: list[str], augmented: bool = False) -> None:\n",
    "    '''Generate all visualizations for a file.'''\n",
    "    for (i, task) in enumerate(OUTPUT_NAMES):\n",
    "        print(f'TASK: {task}')\n",
    "        generate(filename, INPUT_NAME, names, task, i, augmented)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
