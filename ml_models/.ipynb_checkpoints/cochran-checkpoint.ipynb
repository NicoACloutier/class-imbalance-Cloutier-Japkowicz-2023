{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e5eb63e-3523-4bbd-9a97-dfec7496c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import cochrans_q\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6cc2c8-6530-4466-8f4e-2359abdb9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    'binary': ['any', 0, 1], \n",
    "    #'type': ['any', 0, 1, 2, 3]\n",
    "}\n",
    "representations = ['bow', 'freq', 'tfidf']\n",
    "models = ['decision-tree', 'svm', 'naive-bayes', 'naive-bayes-multinomial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68793fb8-448f-44a0-b700-0162a1873a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mode of a numpy array\n",
    "def mode(array):\n",
    "    freq = np.bincount(array)\n",
    "    return np.argmax(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fd1d76-08ec-486d-8d49-f5ff2b77eec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combine columns in a dataframe whose names contain certain substrings\n",
    "def combine_columns(df, substrings):\n",
    "    '''\n",
    "    df: the input dataframe that has many column names that contain certain common substrings\n",
    "    substrings: the list of substrings used for combination\n",
    "    \n",
    "    returns: `output_df`, a DataFrame with `substrings` as its columns. Each row is the mode\n",
    "    of the value for that row of all of the columns in `df` that contained that substring.\n",
    "    '''\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    for column_substring in substrings:\n",
    "        temp_df = pd.DataFrame()\n",
    "        to_be_combined = [column for column in df.columns.values if column_substring in column]\n",
    "        \n",
    "        for column in to_be_combined:\n",
    "            temp_df[column] = df[column]\n",
    "            \n",
    "        array = np.array(temp_df).astype(int)\n",
    "        array = [mode(row) for row in array]\n",
    "            \n",
    "        output_df[column_substring] = array\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9170929b-2fae-45d6-9244-a4c955dd86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find which models performed best using basic accuracy score\n",
    "def evaluate_models(df, substrings):\n",
    "    output_df = pd.DataFrame()\n",
    "    temp_df = combine_columns(df, substrings)\n",
    "    for column in substrings:\n",
    "        temp_df[column] = temp_df[column] == df['Actual']\n",
    "        output_df[column] = [sum(temp_df[column])/len(temp_df),]\n",
    "    output_df.index = ['Score']\n",
    "    return output_df\n",
    "\n",
    "#test model significance difference w/ Friedman Chi^2 while combining certain columns\n",
    "def test_model_difference(df, substrings):\n",
    "    df = combine_columns(df, substrings)\n",
    "    columns = [df[substring] for substring in substrings]\n",
    "    friedman_result = cochrans_q(*columns)\n",
    "    return friedman_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b3415c-0ebe-4943-8d45-68c7ebf4a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find specific rankings for each model\n",
    "def overall_evaluation(task, classification):\n",
    "    df = pd.read_csv(f'{task}_predictions.csv')\n",
    "    if classification != 'any':\n",
    "        df = df[df['Actual'] == classification]\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    temp_df = evaluate_models(df, df.columns.values).transpose()\n",
    "    temp_df = temp_df.sort_values('Score', ascending=False)\n",
    "    friedman_result = test_model_difference(df, df.columns.values[1:])\n",
    "    print(f'Task: {task}.')\n",
    "    print(f'Friedman chi square test p-value: {friedman_result.pvalue}')\n",
    "    print(temp_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b39c89a-eb6a-4782-98dc-6f70275246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate algorithms and representations, find significance level\n",
    "def algorithm_and_rep_eval(task, classification):\n",
    "    df = pd.read_csv(f'{task}_predictions.csv')\n",
    "    if classification != 'any':\n",
    "        df = df[df['Actual'] == classification]\n",
    "        df = df.reset_index()\n",
    "    for grouping in [representations, models]:\n",
    "        temp_df = evaluate_models(df, grouping).transpose()\n",
    "        temp_df = temp_df.sort_values('Score', ascending=False)\n",
    "        friedman_result = test_model_difference(df, grouping)\n",
    "        temp_df.columns = ['Score']\n",
    "        \n",
    "        print(f'Task: {task}. Grouping: {grouping}.')\n",
    "        print(f'Friedman chi square test p-value: {friedman_result.pvalue}')\n",
    "        print(temp_df)\n",
    "        print(f'Mean: {temp_df[\"Score\"].mean()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce76f7d8-e806-49f5-a705-c65baa05c395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: Binary\n",
      "\n",
      "Classification: any\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'statsmodels.stats' has no attribute 'contingency_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classification \u001b[38;5;129;01min\u001b[39;00m tasks[task]:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassification: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassification\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43moverall_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#find the rankings for all models\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     algorithm_and_rep_eval(task, classification)\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36moverall_evaluation\u001b[1;34m(task, classification)\u001b[0m\n\u001b[0;32m      8\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m evaluate_models(df, df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m      9\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m temp_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m friedman_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFriedman chi square test p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfriedman_result\u001b[38;5;241m.\u001b[39mpvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mtest_model_difference\u001b[1;34m(df, substrings)\u001b[0m\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m combine_columns(df, substrings)\n\u001b[0;32m     14\u001b[0m columns \u001b[38;5;241m=\u001b[39m [df[substring] \u001b[38;5;28;01mfor\u001b[39;00m substring \u001b[38;5;129;01min\u001b[39;00m substrings]\n\u001b[1;32m---> 15\u001b[0m friedman_result \u001b[38;5;241m=\u001b[39m \u001b[43mstatsmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontingency_tables\u001b[49m\u001b[38;5;241m.\u001b[39mcochrans_q(\u001b[38;5;241m*\u001b[39mcolumns)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m friedman_result\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'statsmodels.stats' has no attribute 'contingency_tables'"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    print(f'\\nTask: {task.title()}\\n')\n",
    "    for classification in tasks[task]:\n",
    "        print(f'Classification: {classification}')\n",
    "        overall_evaluation(task, classification) #find the rankings for all models\n",
    "        algorithm_and_rep_eval(task, classification) #group representations and algorithms together, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626fb47-45a6-4c7b-9312-506aa5c35a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
